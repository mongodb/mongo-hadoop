import org.apache.tools.ant.taskdefs.condition.Os

buildscript {
    repositories {
        mavenCentral()
    }
    dependencies {
        classpath group: 'org.zeroturnaround', name: 'zt-exec', version: '1.6'
    }
}

def execute(command, args = [], outStream = null, errStream = null, background = false ) {
    def env = [HADOOP_HOME: hadoopHome]
    env << System.getProperties()

    def executor = new org.zeroturnaround.exec.ProcessExecutor().command([command.toString()] + args)
                                                                .readOutput(true)
                                                                .environment(env)
                                                                .redirectOutput(outStream)
                                                                .redirectError(errStream != null ? errStream : outStream);
    if (!background) {
        try {
            def result = executor.execute();
            if (outStream == null) {
                return result.outputString().split('\n')
            }
        } catch (Exception e) {
            e.printStackTrace()
        }
    } else {
        executor.start()
    }
}

def stopService(signal, service, name) {
    for (String process : execute('jps')) {
        if (process.endsWith(service)) {
            println("Shutting down ${name}")
            if (Os.isFamily(Os.FAMILY_WINDOWS)) {
                if (signal == 'TERM') {
                    execute("taskkill", "/PID", process.split()[0])
                 } else{
                    execute("taskkill", "/F", "/PID", process.split()[0])
                }
            }else{
                if (signal == 'TERM'){
                    execute("kill", process.split()[0])
                } else {
                    execute("kill", "-9", process.split()[0])
                }
            }
        }
    }
}

def startService(cmd, service) {
    println "Starting ${service}"

    execute("$hadoopHome/bin/${cmd}", [service], new FileOutputStream("${rootDir}/build/logs/${service}.log"), null, true)
}

def createHiveWarehouse() {
    print('Creating Hive warehouse directory')
    execute("$hadoopHome/bin/hadoop", ["fs", "-mkdir", "-p", "hdfs:///user/hive/warehouse"], new FileOutputStream
            ("${rootDir}/build/logs/hiveserver-mkdir.out"))
    execute("$hadoopHome/bin/hadoop", ["fs", "-chmod", "g+w", "hdfs:///user/hive/warehouse"], new FileOutputStream
            ("${rootDir}/build/logs/hiveserver-chmod.out"))
}


def startYarn() {
    startService('hdfs', 'namenode')
    sleep(5000)
    startService('hdfs', 'datanode')
    startService('yarn', 'resourcemanager')
    startService('yarn', 'nodemanager')
    createHiveWarehouse()
}

def startHadoop() {
    println "Starting HDFS"
    // Use hadoop-deamon.sh directly instead of wrapper scripts to avoid
    // needing to SSH in Jenkins.
    execute("${hadoopHome}/bin/hadoop-daemon.sh", ["start", "namenode"],
            new FileOutputStream("${rootDir}/build/logs/start-namenode.out"))
    execute("${hadoopHome}/bin/hadoop-daemon.sh", ["start", "datanode"],
            new FileOutputStream("${rootDir}/build/logs/start-datanode.out"))
    println "Starting MapReduce"
    execute("${hadoopHome}/bin/hadoop-daemon.sh", ["start", "jobtracker"],
            new FileOutputStream("${rootDir}/build/logs/start-jobtracker.out"))
    sleep(3000)
    execute("${hadoopHome}/bin/hadoop-daemon.sh", ["start", "tasktracker"],
            new FileOutputStream("${rootDir}/build/logs/start-tasktracker.out"))
    createHiveWarehouse()
}

def stopAll(signal) {
    println "shutting down Hadoop"
    stopService(signal, 'NodeManager', 'node manager')
    stopService(signal, 'ResourceManager', 'resource manager')
    stopService(signal, 'DataNode', 'data node')
    stopService(signal, 'NameNode', 'name node')
    stopService(signal, 'RunJar', 'hive server')
    stopService(signal, 'JobTracker', 'job tracker')
    stopService(signal, 'TaskTracker', 'task tracker')
}

task shutdownCluster() << {
    println "shutdownCluster"
    stopAll('TERM')
    stopAll('KILL')
}

task startCluster(dependsOn: [copyFiles]) << {
    if (!file("${rootDir}/build/logs/").isDirectory()) {
        file("${rootDir}/build/logs/").mkdirs()
    }

    delete file("${hadoopBinaries}/hadoop-tmpdir")

    println "Formatting HDFS"
    def hdfs
    if (isHadoopV1) {
        hdfs = "${hadoopHome}/bin/hadoop"
    } else {
        hdfs = "${hadoopHome}/bin/hdfs"
    }
    execute(hdfs, ['namenode', '-format', '-force'], null,
            new FileOutputStream("${rootDir}/build/logs/namenode-format.out"))

    if (isHadoopV1) {
        startHadoop()
    } else {
        startYarn()
    }
}

task historicalYield(dependsOn: startCluster) << {

    exec() {
        commandLine mongoimport, "-d", "mongo_hadoop", "-c", "yield_historical.in", "--drop",
                    "examples/treasury_yield/src/main/resources/yield_historical_in.json"
    }

    hadoop("examples/treasury_yield/build/libs/treasury_yield-${project(':core').version}.jar",
           "com.mongodb.hadoop.examples.treasury.TreasuryYieldXMLConfig", [
            "mongo.input.uri=mongodb://localhost:27017/mongo_hadoop.yield_historical.in",
            "mongo.output.uri=mongodb://localhost:27017/mongo_hadoop.yield_historical.out"
    ])
}

task sensorData(dependsOn: 'startCluster') << {
    hadoop("examples/sensors/build/libs/sensors-${project(':core').version}.jar",
           "com.mongodb.hadoop.examples.sensors.Devices", [])

    hadoop("examples/sensors/build/libs/sensors-${project(':core').version}.jar",
           "com.mongodb.hadoop.examples.sensors.Logs", ["io.sort.mb=100"])
}


task enronEmails(dependsOn: [downloadEnronEmails, startCluster]) << {
    // Create BSON file input directory.
    exec() {
        commandLine "${hadoopHome}/bin/hdfs", "dfs", "-mkdir", "-p", "/messages"
    }
    // Split messages.bson and upload to HDFS.
    hadoop("core/build/libs/mongo-hadoop-core-${project(':core').version}.jar",
            "com.mongodb.hadoop.splitter.BSONSplitter", [],
            ["file://${dataHome}/dump/enron_mail/messages.bson",
             "-c", "org.apache.hadoop.io.compress.BZip2Codec",
             "-o", "hdfs://localhost:8020/messages"])
    // MR job
    hadoop("examples/enron/build/libs/enron-${project(':core').version}.jar",
           "com.mongodb.hadoop.examples.enron.EnronMail", [])
}

task shakespeare(dependsOn: [downloadShakespeare, startCluster]) << {
    // Split up complete works by title and upload to GridFS.
    def filePath = new File(System.properties['java.io.tmpdir'], "pg100.txt")
    hadoop("examples/shakespeare/build/libs/shakespeare-${project(':core').version}.jar",
            "com.mongodb.hadoop.examples.shakespeare.PrepareShakespeare", [],
            [filePath.toString(),
             "mongodb://localhost:27017/mongo_hadoop.shakespeare"])

    // MR job
    hadoop("examples/shakespeare/build/libs/shakespeare-${project(':core').version}.jar",
            "com.mongodb.hadoop.examples.shakespeare.Shakespeare", [])
}

def hadoop(jar, className, args, commandArgs = []) {
    def line = ["${hadoopHome}/bin/hadoop",
                "jar", jar, className,
                //Split settings
                "-Dmongo.input.split_size=8",
                "-Dmongo.job.verbose=true",
    ]
    // Allow overriding the input URI with an environment variable.
    if (mongoURI != null) {
        line.add("-Dmongo.input.uri=${mongoURI}")
    }
    if (mongoAuthURI != null) {
        line.add("-Dmongo.auth.uri=${mongoAuthURI}")
    }
    args.each {
        line << "-D${it}"
    }
    commandArgs.each {
        line << "${it}"
    }
    println "Executing hadoop job:\n ${line.join(' \\\n\t')}"
    def hadoopEnv = [:]
    hadoopEnv.HADOOP_PREFIX=''
    exec() {
        environment << hadoopEnv
        commandLine line
    }
}
