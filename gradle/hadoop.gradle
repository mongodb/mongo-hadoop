import org.apache.tools.ant.taskdefs.condition.Os

buildscript {
    repositories {
        mavenCentral()
    }
    dependencies {
        classpath group: 'org.zeroturnaround', name: 'zt-exec', version: '1.6'
    }
}

def execute(command, args = [], outStream = null, errStream = null, background = false ) {
    def env = [HADOOP_HOME: hadoopHome]
    env << System.getProperties()
    
    def executor = new org.zeroturnaround.exec.ProcessExecutor().command([command.toString()] + args)
                                                                .readOutput(true)
                                                                .environment(env)
                                                                .redirectOutput(outStream)
                                                                .redirectError(errStream != null ? errStream : outStream);
    if (!background) {
        try {
            def result = executor.execute();
            if (outStream == null) {
                return result.outputString().split('\n')
            }
        } catch (Exception e) {
            e.printStackTrace()
        }
    } else {
        executor.start()
    }
}

def stopService(signal, service, name) {
    for (String process : execute('jps')) {
        if (process.endsWith(service)) {
            println("Shutting down ${name}")
            if (Os.isFamily(Os.FAMILY_WINDOWS)) {
                if (signal == 'TERM') {
                    execute("taskkill", "/PID", process.split()[0])
                 } else{
                    execute("taskkill", "/F", "/PID", process.split()[0])
                }
            }else{
                if (signal == 'TERM'){
                    execute("kill", process.split()[0])
                } else {
                    execute("kill", "-9", process.split()[0])
                }
            }
        }
    }
}

def startService(cmd, service) {
    println "Starting ${service}"

    execute("$hadoopHome/bin/${cmd}", [service], new FileOutputStream("${rootDir}/build/logs/${service}.log"), null, true)
}

def start() {
    startService('hdfs', 'namenode')
    sleep(5000)
    startService('hdfs', 'datanode')
    startService('yarn', 'resourcemanager')
    startService('yarn', 'nodemanager')

    print('Starting hiveserver') 
    execute("$hadoopHome/bin/hadoop", ["fs", "-mkdir", "-p", "hdfs:///user/hive/warehouse"], new FileOutputStream
            ("${rootDir}/build/logs/hiveserver-mkdir.out"))
    execute("$hadoopHome/bin/hadoop", ["fs", "-chmod", "g+w", "hdfs:///user/hive/warehouse"], new FileOutputStream
                ("${rootDir}/build/logs/hiveserver-chmod.out"))

    execute("$hiveHome/bin/hive", ["--service", "hiveserver"], new FileOutputStream("${rootDir}/build/logs/hiveserver.log"), null, true)
}

def stopAll(signal) {
    stopService(signal, 'NodeManager', 'node manager')
    stopService(signal, 'ResourceManager', 'resource manager')
    stopService(signal, 'DataNode', 'data node')
    stopService(signal, 'NameNode', 'name node')
    stopService(signal, 'RunJar', 'hive server')
}

task shutdownCluster() << {
    stopAll('TERM')
    stopAll('KILL')
}

task startCluster(dependsOn: [copyFiles]) << {
    if (!file("${rootDir}/build/logs/").isDirectory()) {
        file("${rootDir}/build/logs/").mkdirs()
    }

    delete {
        delete fileTree(dir: file("${hadoopBinaries}/hadoop-tmpdir/"))
    }
    println "Formatting HDFS"
    execute("${hadoopHome}/bin/hdfs", [ 'namenode', '-format', '-force' ], null,
            new FileOutputStream("${rootDir}/build/logs/namenode-format.out"))

    start()
}

task historicalYield(dependsOn: startCluster) << {
    
    exec() {
        commandLine mongoimport, "-d", "mongo_hadoop", "-c", "yield_historical.in", "--drop",
                    "examples/treasury_yield/src/main/resources/yield_historical_in.json"
    }

    hadoop("examples/treasury_yield/build/libs/treasury_yield-${project(':core').version}.jar",
           "com.mongodb.hadoop.examples.treasury.TreasuryYieldXMLConfig", [
            "mongo.input.uri=mongodb://localhost:27017/mongo_hadoop.yield_historical.in",
            "mongo.output.uri=mongodb://localhost:27017/mongo_hadoop.yield_historical.out"
    ])
}

task sensorData(dependsOn: 'startCluster') << {
    hadoop("examples/sensors/build/libs/sensors-${project(':core').version}.jar",
           "com.mongodb.hadoop.examples.sensors.Devices", [])

    hadoop("examples/sensors/build/libs/sensors-${project(':core').version}.jar",
           "com.mongodb.hadoop.examples.sensors.Logs", ["io.sort.mb=100"])
}


task enronEmails(dependsOn: [downloadEnronEmails, startCluster]) << {
    exec() {
        commandLine mongorestore, "-v", "-d", "mongo_hadoop", "--drop", "${dataHome}/dump/enron_mail"
    }

    hadoop("examples/enron/build/libs/enron-${project(':core').version}.jar",
           "com.mongodb.hadoop.examples.enron.EnronMail", ["mongo.input.split_size=64"])
}

def hadoop(jar, className, args) {
    def line = ["${hadoopHome}/bin/hadoop",
                "jar", jar, className,
                //Split settings
                "-Dmongo.input.split_size=8",
                "-Dmongo.job.verbose=true",
    ]
    args.each {
        line << "-D${it}"
    }
    println "Executing hadoop job:\n ${line.join(' \\\n\t')}"
    def hadoopEnv = [:]
    hadoopEnv.HADOOP_PREFIX=''
    exec() {
        environment << hadoopEnv
        commandLine line
    }
}